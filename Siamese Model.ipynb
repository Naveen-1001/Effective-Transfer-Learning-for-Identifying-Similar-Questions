{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "909e7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3017a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b277d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import preprocessing\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93d3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1411da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719d8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    question1 = df['question1'].astype(str).values\n",
    "    question2 = df['question2'].astype(str).values\n",
    "    # combined: to get the tokens\n",
    "    df['combined'] = df['question1'] + df['question2']\n",
    "    labels = df['is_duplicate'].values\n",
    "    return question1, question2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2c7ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is Pulsar 220?</td>\n",
       "      <td>Is Pulsar 220 good?</td>\n",
       "      <td>1</td>\n",
       "      <td>How is Pulsar 220?Is Pulsar 220 good?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does GoldFlake get the smoothest cigarette...</td>\n",
       "      <td>What are the best cigarettes you can get at 7-11?</td>\n",
       "      <td>0</td>\n",
       "      <td>How does GoldFlake get the smoothest cigarette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I increase my focus in study?</td>\n",
       "      <td>How do we increase concentration?</td>\n",
       "      <td>1</td>\n",
       "      <td>How I increase my focus in study?How do we inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to do biotechnology from abroad which c...</td>\n",
       "      <td>Why doesn’t Sterling Immigration update their ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I want to do biotechnology from abroad which c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0                                 How is Pulsar 220?   \n",
       "1  How much marks one need to score in GATE to ge...   \n",
       "2  How does GoldFlake get the smoothest cigarette...   \n",
       "3                  How I increase my focus in study?   \n",
       "4  I want to do biotechnology from abroad which c...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0                                Is Pulsar 220 good?             1   \n",
       "1  How much marks one need to score in GATE to ge...             0   \n",
       "2  What are the best cigarettes you can get at 7-11?             0   \n",
       "3                  How do we increase concentration?             1   \n",
       "4  Why doesn’t Sterling Immigration update their ...             0   \n",
       "\n",
       "                                            combined  \n",
       "0              How is Pulsar 220?Is Pulsar 220 good?  \n",
       "1  How much marks one need to score in GATE to ge...  \n",
       "2  How does GoldFlake get the smoothest cigarette...  \n",
       "3  How I increase my focus in study?How do we inc...  \n",
       "4  I want to do biotechnology from abroad which c...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('QQP.csv')\n",
    "question1, question2, labels = load_data(df)\n",
    "question1 = list(question1)\n",
    "question2 = list(question2)\n",
    "combined = question1 + question2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92af15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAscii(text):\n",
    "       return ''.join(i for i in text if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14dd35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8c200e623f49>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined'][ind] = cleanAscii(df['combined'][ind])\n"
     ]
    }
   ],
   "source": [
    "for ind in df.index:\n",
    "    df['combined'][ind] = cleanAscii(df['combined'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8309f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is Pulsar 220?</td>\n",
       "      <td>Is Pulsar 220 good?</td>\n",
       "      <td>1</td>\n",
       "      <td>How is Pulsar 220?Is Pulsar 220 good?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>How much marks one need to score in GATE to ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does GoldFlake get the smoothest cigarette...</td>\n",
       "      <td>What are the best cigarettes you can get at 7-11?</td>\n",
       "      <td>0</td>\n",
       "      <td>How does GoldFlake get the smoothest cigarette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I increase my focus in study?</td>\n",
       "      <td>How do we increase concentration?</td>\n",
       "      <td>1</td>\n",
       "      <td>How I increase my focus in study?How do we inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to do biotechnology from abroad which c...</td>\n",
       "      <td>Why doesn’t Sterling Immigration update their ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I want to do biotechnology from abroad which c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0                                 How is Pulsar 220?   \n",
       "1  How much marks one need to score in GATE to ge...   \n",
       "2  How does GoldFlake get the smoothest cigarette...   \n",
       "3                  How I increase my focus in study?   \n",
       "4  I want to do biotechnology from abroad which c...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0                                Is Pulsar 220 good?             1   \n",
       "1  How much marks one need to score in GATE to ge...             0   \n",
       "2  What are the best cigarettes you can get at 7-11?             0   \n",
       "3                  How do we increase concentration?             1   \n",
       "4  Why doesn’t Sterling Immigration update their ...             0   \n",
       "\n",
       "                                            combined  \n",
       "0              How is Pulsar 220?Is Pulsar 220 good?  \n",
       "1  How much marks one need to score in GATE to ge...  \n",
       "2  How does GoldFlake get the smoothest cigarette...  \n",
       "3  How I increase my focus in study?How do we inc...  \n",
       "4  I want to do biotechnology from abroad which c...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tok.fit_on_texts(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1840bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(combined)\n",
    "sequences = preprocessing.sequence.pad_sequences(sequences, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce3eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n",
      "24384\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "word_index = len(tok.word_index) + 1\n",
    "glove_dir = ''\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(word_index)\n",
    "# matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tok.word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94397a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01502a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "# loading our matrix\n",
    "emb = tf.keras.layers.Embedding(max_words, embedding_dim, input_length=300, weights=[embedding_matrix],trainable=False)\n",
    "input1 = tf.keras.Input(shape=(300,))\n",
    "e1 = emb(input1)\n",
    "x1 = lstm_layer(e1)\n",
    "input2 = tf.keras.Input(shape=(300,))\n",
    "e2 = emb(input2)\n",
    "x2 = lstm_layer(e2)\n",
    "mhd = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "merged = tf.keras.layers.Lambda(function=mhd, output_shape=lambda x: x[0],name='L1_distance')([x1, x2])\n",
    "preds = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "model = tf.keras.Model(inputs=[input1, input2], outputs=preds)\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c43eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    features, labels = df.drop(columns=['is_duplicate']).values, df['is_duplicate'].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20daa82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, x_val, y_val = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "11dc73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit([x_train[:,0], x_train[:,1]], y_train, epochs=100, validation_data=([x_val[:,0], x_val[:,1]], y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2c8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd66171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d772783",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "\n",
    "RATE_DROP_LSTM = 0.17\n",
    "RATE_DROP_DENSE = 0.25\n",
    "NUMBER_LSTM = 50\n",
    "NUMBER_DENSE_UNITS = 50\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "\n",
    "siamese_config = {\n",
    "\t'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "\t'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH,\n",
    "\t'VALIDATION_SPLIT': VALIDATION_SPLIT,\n",
    "\t'RATE_DROP_LSTM': RATE_DROP_LSTM,\n",
    "\t'RATE_DROP_DENSE': RATE_DROP_DENSE,\n",
    "\t'NUMBER_LSTM': NUMBER_LSTM,\n",
    "\t'NUMBER_DENSE_UNITS': NUMBER_DENSE_UNITS,\n",
    "\t'ACTIVATION_FUNCTION': ACTIVATION_FUNCTION\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48fa9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "def train_word2vec(documents, embedding_dim):\n",
    "    model = Word2Vec(documents, min_count=1, vector_size=embedding_dim)\n",
    "    word_vectors = model.wv\n",
    "    del model\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "def create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n",
    "    nb_words = len(tokenizer.word_index) + 1\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"vector not found for word - %s\" % word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def word_embed_meta_data(documents, embedding_dim):\n",
    "    documents = [x.lower().split() for x in documents]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(documents)\n",
    "    word_vector = train_word2vec(documents, embedding_dim)\n",
    "    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n",
    "    del word_vector\n",
    "    gc.collect()\n",
    "    return tokenizer, embedding_matrix\n",
    "\n",
    "\n",
    "def create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):eaks_val (np.array):\n",
    "    sentences1 = [x[0].lower() for x in sentences_pair]\n",
    "    sentences2 = [x[1].lower() for x in sentences_pair]\n",
    "    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\n",
    "    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\n",
    "    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
    "             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\n",
    "\n",
    "    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n",
    "    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n",
    "    train_labels = np.array(is_similar)\n",
    "    leaks = np.array(leaks)\n",
    "\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n",
    "    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\n",
    "    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\n",
    "    train_labels_shuffled = train_labels[shuffle_indices]\n",
    "    leaks_shuffled = leaks[shuffle_indices]\n",
    "\n",
    "    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\n",
    "\n",
    "    del train_padded_data_1\n",
    "    del train_padded_data_2\n",
    "    gc.collect()\n",
    "\n",
    "    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\n",
    "    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\n",
    "    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\n",
    "    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\n",
    "\n",
    "    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\n",
    "\n",
    "\n",
    "def create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\n",
    "    test_sentences1 = [x[0].lower() for x in test_sentences_pair]\n",
    "    test_sentences2 = [x[1].lower() for x in test_sentences_pair]\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\n",
    "    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\n",
    "    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
    "                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\n",
    "\n",
    "    leaks_test = np.array(leaks_test)\n",
    "    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n",
    "    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n",
    "\n",
    "    return test_data_1, test_data_2, leaks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbe549ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "\n",
    "# std imports\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SiameseBiLSTM:\n",
    "    def __init__(self, embedding_dim, max_sequence_length, number_lstm, number_dense, rate_drop_lstm, \n",
    "                 rate_drop_dense, hidden_activation, validation_split_ratio):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.number_lstm_units = number_lstm\n",
    "        self.rate_drop_lstm = rate_drop_lstm\n",
    "        self.number_dense_units = number_dense\n",
    "        self.activation_function = hidden_activation\n",
    "        self.rate_drop_dense = rate_drop_dense\n",
    "        self.validation_split_ratio = validation_split_ratio\n",
    "\n",
    "    def train_model(self, sentences_pair, is_similar, embedding_meta_data, model_save_directory='./'):urn (best_model_path):\n",
    "        tokenizer, embedding_matrix = embedding_meta_data['tokenizer'], embedding_meta_data['embedding_matrix']\n",
    "\n",
    "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
    "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\n",
    "                                                                               is_similar, self.max_sequence_length,\n",
    "                                                                               self.validation_split_ratio)\n",
    "\n",
    "        if train_data_x1 is None:\n",
    "            print(\"++++ !! Failure: Unable to train model ++++\")\n",
    "            return None\n",
    "\n",
    "        nb_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # Creating word embedding layer\n",
    "        embedding_layer = Embedding(nb_words, self.embedding_dim, weights=[embedding_matrix],\n",
    "                                    input_length=self.max_sequence_length, trainable=False)\n",
    "\n",
    "        # Creating LSTM Encoder\n",
    "        lstm_layer = Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "\n",
    "        # Creating LSTM Encoder layer for First Sentence\n",
    "        sequence_1_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        # Creating LSTM Encoder layer for Second Sentence\n",
    "        sequence_2_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        x2 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        # Creating leaks input\n",
    "        leaks_input = Input(shape=(leaks_train.shape[1],))\n",
    "        leaks_dense = Dense(int(self.number_dense_units/2), activation=self.activation_function)(leaks_input)\n",
    "\n",
    "        # Merging two LSTM encodes vectors from sentences to\n",
    "        # pass it to dense layer applying dropout and batch normalisation\n",
    "        merged = concatenate([x1, x2, leaks_dense])\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(self.rate_drop_dense)(merged)\n",
    "        merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dropout(self.rate_drop_dense)(merged)\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input, leaks_input], outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "        STAMP = 'lstm_%d_%d_%.2f_%.2f' % (self.number_lstm_units, self.number_dense_units, self.rate_drop_lstm, self.rate_drop_dense)\n",
    "\n",
    "        checkpoint_dir = model_save_directory + 'checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        bst_model_path = checkpoint_dir + STAMP + '.h5'\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "        tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n",
    "\n",
    "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
    "                  epochs=200, batch_size=64, shuffle=True,\n",
    "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
    "\n",
    "        return bst_model_path\n",
    "\n",
    "\n",
    "    def update_model(self, saved_model_path, new_sentences_pair, is_similar, embedding_meta_data):\n",
    "        tokenizer = embedding_meta_data['tokenizer']\n",
    "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
    "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, new_sentences_pair,\n",
    "                                                                               is_similar, self.max_sequence_length,\n",
    "                                                                               self.validation_split_ratio)\n",
    "        model = load_model(saved_model_path)\n",
    "        model_file_name = saved_model_path.split('/')[-1]\n",
    "        new_model_checkpoint_path  = saved_model_path.split('/')[:-2] + str(int(time.time())) + '/' \n",
    "\n",
    "        new_model_path = new_model_checkpoint_path + model_file_name\n",
    "        model_checkpoint = ModelCheckpoint(new_model_checkpoint_path + model_file_name,\n",
    "                                           save_best_only=True, save_weights_only=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "        tensorboard = TensorBoard(log_dir=new_model_checkpoint_path + \"logs/{}\".format(time.time()))\n",
    "\n",
    "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
    "                  epochs=50, batch_size=3, shuffle=True,\n",
    "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
    "\n",
    "        return new_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ecb789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (39989, 50)\n",
      "Null word embeddings: 1\n",
      "Epoch 1/200\n",
      "330/330 [==============================] - 23s 25ms/step - loss: 0.6356 - acc: 0.6481 - val_loss: 0.5279 - val_acc: 0.7200\n",
      "Epoch 2/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.5360 - acc: 0.7046 - val_loss: 0.5036 - val_acc: 0.7222\n",
      "Epoch 3/200\n",
      "330/330 [==============================] - 7s 21ms/step - loss: 0.5154 - acc: 0.7219 - val_loss: 0.4918 - val_acc: 0.7371\n",
      "Epoch 4/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.5041 - acc: 0.7240 - val_loss: 0.4862 - val_acc: 0.7443\n",
      "Epoch 5/200\n",
      "330/330 [==============================] - 7s 23ms/step - loss: 0.4977 - acc: 0.7319 - val_loss: 0.4837 - val_acc: 0.7375\n",
      "Epoch 6/200\n",
      "330/330 [==============================] - 7s 21ms/step - loss: 0.4917 - acc: 0.7364 - val_loss: 0.4772 - val_acc: 0.7473\n",
      "Epoch 7/200\n",
      "330/330 [==============================] - 7s 21ms/step - loss: 0.4884 - acc: 0.7367 - val_loss: 0.4821 - val_acc: 0.7405\n",
      "Epoch 8/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.4862 - acc: 0.7388 - val_loss: 0.4858 - val_acc: 0.7367\n",
      "Epoch 9/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.4837 - acc: 0.7461 - val_loss: 0.4757 - val_acc: 0.7495\n",
      "Epoch 10/200\n",
      "330/330 [==============================] - 7s 21ms/step - loss: 0.4769 - acc: 0.7466 - val_loss: 0.4800 - val_acc: 0.7478\n",
      "Epoch 11/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.4784 - acc: 0.7511 - val_loss: 0.4832 - val_acc: 0.7443\n",
      "Epoch 12/200\n",
      "330/330 [==============================] - 8s 23ms/step - loss: 0.4736 - acc: 0.7497 - val_loss: 0.4711 - val_acc: 0.7563\n",
      "Epoch 13/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4689 - acc: 0.7565 - val_loss: 0.4687 - val_acc: 0.7461\n",
      "Epoch 14/200\n",
      "330/330 [==============================] - 7s 22ms/step - loss: 0.4659 - acc: 0.7570 - val_loss: 0.4776 - val_acc: 0.7456\n",
      "Epoch 15/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4640 - acc: 0.7594 - val_loss: 0.4678 - val_acc: 0.7507\n",
      "Epoch 16/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4565 - acc: 0.7626 - val_loss: 0.4685 - val_acc: 0.7465\n",
      "Epoch 17/200\n",
      "330/330 [==============================] - 8s 23ms/step - loss: 0.4587 - acc: 0.7601 - val_loss: 0.4738 - val_acc: 0.7401\n",
      "Epoch 18/200\n",
      "330/330 [==============================] - 8s 23ms/step - loss: 0.4560 - acc: 0.7658 - val_loss: 0.4673 - val_acc: 0.7610\n",
      "Epoch 19/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4501 - acc: 0.7719 - val_loss: 0.4725 - val_acc: 0.7465\n",
      "Epoch 20/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4479 - acc: 0.7670 - val_loss: 0.4728 - val_acc: 0.7456\n",
      "Epoch 21/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4469 - acc: 0.7731 - val_loss: 0.4654 - val_acc: 0.7546\n",
      "Epoch 22/200\n",
      "330/330 [==============================] - 8s 24ms/step - loss: 0.4455 - acc: 0.7735 - val_loss: 0.4688 - val_acc: 0.7499\n",
      "Epoch 23/200\n",
      "330/330 [==============================] - 8s 25ms/step - loss: 0.4391 - acc: 0.7792 - val_loss: 0.4688 - val_acc: 0.7443\n",
      "Epoch 24/200\n",
      "330/330 [==============================] - 8s 25ms/step - loss: 0.4361 - acc: 0.7801 - val_loss: 0.4681 - val_acc: 0.7529\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[('What can make Physics easy to learn?', 'How can you make physics easy to learn?', 0.5891942), ('How many times a day do a clocks hands overlap?', 'What does it mean that every time I look at the clock the numbers are the same?', 0.0001270175)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "########################################\n",
    "############ Data Preperation ##########\n",
    "########################################\n",
    "\n",
    "\n",
    "df = pd.read_csv('QQP.csv')\n",
    "\n",
    "sentences1 = list(df['question1'])\n",
    "sentences2 = list(df['question2'])\n",
    "is_similar = list(df['is_duplicate'])\n",
    "del df\n",
    "\n",
    "\n",
    "####################################\n",
    "######## Word Embedding ############\n",
    "####################################\n",
    "\n",
    "\n",
    "# creating word embedding meta data for word embedding \n",
    "tokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'])\n",
    "\n",
    "embedding_meta_data = {\n",
    "\t'tokenizer': tokenizer,\n",
    "\t'embedding_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "## creating sentence pairs\n",
    "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
    "del sentences1\n",
    "del sentences2\n",
    "\n",
    "\n",
    "##########################\n",
    "######## Training ########\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "class Configuration(object):\n",
    "    \"\"\"Dump stuff here\"\"\"\n",
    "\n",
    "CONFIG = Configuration()\n",
    "CONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\n",
    "CONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\n",
    "CONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\n",
    "CONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\n",
    "CONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\n",
    "CONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\n",
    "CONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\n",
    "CONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n",
    "\n",
    "siamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, \n",
    "\t\t\t\t\t    CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n",
    "\n",
    "best_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='./')\n",
    "\n",
    "\n",
    "########################\n",
    "###### Testing #########\n",
    "########################\n",
    "\n",
    "# model = load_model(best_model_path)\n",
    "\n",
    "# test_sentence_pairs = [('What can make Physics easy to learn?','How can you make physics easy to learn?'),\n",
    "# \t\t\t\t\t   ('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n",
    "\n",
    "# test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
    "\n",
    "# preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
    "# results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
    "# results.sort(key=itemgetter(2), reverse=True)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e497145",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqp = pd.read_csv('mqp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da316a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After how many hour from drinking an antibioti...</td>\n",
       "      <td>I vomited this morning and I am not sure if it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>I am a 39 y/o male currently weighing about 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>What diet is good for losing weight? Keto or v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>How much Aspirin can I take for my headache wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>My friend told me about this bracelet for Aspi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  After how many hour from drinking an antibioti...   \n",
       "1          Am I over weight (192.9) for my age (39)?   \n",
       "2          Am I over weight (192.9) for my age (39)?   \n",
       "3  Aspirin allergy - is it worth getting a bracelet?   \n",
       "4  Aspirin allergy - is it worth getting a bracelet?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  I vomited this morning and I am not sure if it...             0  \n",
       "1  I am a 39 y/o male currently weighing about 19...             1  \n",
       "2  What diet is good for losing weight? Keto or v...             0  \n",
       "3  How much Aspirin can I take for my headache wi...             0  \n",
       "4  My friend told me about this bracelet for Aspi...             1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4087447",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqp = mqp.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da13042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqp.columns = ['question1','question2','is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f53cc0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After how many hour from drinking an antibioti...</td>\n",
       "      <td>I vomited this morning and I am not sure if it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>I am a 39 y/o male currently weighing about 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>What diet is good for losing weight? Keto or v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>How much Aspirin can I take for my headache wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>My friend told me about this bracelet for Aspi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  After how many hour from drinking an antibioti...   \n",
       "1          Am I over weight (192.9) for my age (39)?   \n",
       "2          Am I over weight (192.9) for my age (39)?   \n",
       "3  Aspirin allergy - is it worth getting a bracelet?   \n",
       "4  Aspirin allergy - is it worth getting a bracelet?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  I vomited this morning and I am not sure if it...             0  \n",
       "1  I am a 39 y/o male currently weighing about 19...             1  \n",
       "2  What diet is good for losing weight? Keto or v...             0  \n",
       "3  How much Aspirin can I take for my headache wi...             0  \n",
       "4  My friend told me about this bracelet for Aspi...             1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fbc5b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "437d856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqp.to_csv('mqp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c44f277c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After how many hour from drinking an antibioti...</td>\n",
       "      <td>I vomited this morning and I am not sure if it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>I am a 39 y/o male currently weighing about 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>What diet is good for losing weight? Keto or v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>How much Aspirin can I take for my headache wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>My friend told me about this bracelet for Aspi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  After how many hour from drinking an antibioti...   \n",
       "1          Am I over weight (192.9) for my age (39)?   \n",
       "2          Am I over weight (192.9) for my age (39)?   \n",
       "3  Aspirin allergy - is it worth getting a bracelet?   \n",
       "4  Aspirin allergy - is it worth getting a bracelet?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  I vomited this morning and I am not sure if it...             0  \n",
       "1  I am a 39 y/o male currently weighing about 19...             1  \n",
       "2  What diet is good for losing weight? Keto or v...             0  \n",
       "3  How much Aspirin can I take for my headache wi...             0  \n",
       "4  My friend told me about this bracelet for Aspi...             1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b83f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv('mqp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee82ab90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After how many hour from drinking an antibioti...</td>\n",
       "      <td>I vomited this morning and I am not sure if it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>I am a 39 y/o male currently weighing about 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am I over weight (192.9) for my age (39)?</td>\n",
       "      <td>What diet is good for losing weight? Keto or v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>How much Aspirin can I take for my headache wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aspirin allergy - is it worth getting a bracelet?</td>\n",
       "      <td>My friend told me about this bracelet for Aspi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  After how many hour from drinking an antibioti...   \n",
       "1          Am I over weight (192.9) for my age (39)?   \n",
       "2          Am I over weight (192.9) for my age (39)?   \n",
       "3  Aspirin allergy - is it worth getting a bracelet?   \n",
       "4  Aspirin allergy - is it worth getting a bracelet?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  I vomited this morning and I am not sure if it...             0  \n",
       "1  I am a 39 y/o male currently weighing about 19...             1  \n",
       "2  What diet is good for losing weight? Keto or v...             0  \n",
       "3  How much Aspirin can I take for my headache wi...             0  \n",
       "4  My friend told me about this bracelet for Aspi...             1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.iloc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
